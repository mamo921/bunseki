import streamlit as st
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import io
from datetime import datetime
import json
import re
import bcrypt
import matplotlib.font_manager as fm # ãƒ•ã‚©ãƒ³ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# ç›¸å¯¾ãƒ‘ã‚¹ã§æŒ‡å®šï¼ˆStreamlit Cloudã§ã‚‚é€šã‚‹ï¼‰
font_path = "bunseki_appv1.0.0git/static/NotoSansJP-VariableFont_wght.otf"
font_prop = fm.FontProperties(fname=font_path)

font_prop = fm.FontProperties(fname=font_path)
plt.rcParams['font.family'] = font_prop.get_name()


# matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
# è² ã®è¨˜å·ãŒæ–‡å­—åŒ–ã‘ã™ã‚‹ã®ã‚’é˜²ã
plt.rcParams['axes.unicode_minus'] = False

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šã‚’ã‚ˆã‚Šå …ç‰¢ã«ã™ã‚‹
# ä¸€èˆ¬çš„ã«Streamlit Cloudã§åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚©ãƒ³ãƒˆã‚’å„ªå…ˆ
japanese_font_available = False
try:
    # Noto Sans CJK JP ã‚’è©¦ã™
    fm.findfont('Noto Sans CJK JP', fallback_to_default=False)
    plt.rcParams['font.family'] = 'Noto Sans CJK JP'
    plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP']
    japanese_font_available = True
except Exception:
    try:
        # IPAexGothic ã‚’è©¦ã™
        fm.findfont('IPAexGothic', fallback_to_default=False)
        plt.rcParams['font.family'] = 'IPAexGothic'
        plt.rcParams['font.sans-serif'] = ['IPAexGothic']
        japanese_font_available = True
    except Exception:
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®è­¦å‘Š
        st.warning("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚°ãƒ©ãƒ•ã®ãƒ©ãƒ™ãƒ«ã¯è‹±èªã§è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        plt.rcParams['font.family'] = 'sans-serif'
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans'] # ä¸€èˆ¬çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

# å…¨ä½“çš„ãªãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰
plt.rcParams['font.size'] = 10

# Helper function for localization (UI elements always in Japanese)
def get_localized_text(jp_text, en_text):
    return jp_text

# Helper function for graph localization (graphs switch to English if Japanese font not available)
def get_graph_text(jp_text, en_text):
    return jp_text


# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title=get_localized_text("VRã‚¤ãƒ™ãƒ³ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«", "VR Event Analysis Tool"), layout="wide")

# --- èªè¨¼é–¢é€£ã®é–¢æ•° ---
# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã®æ¤œè¨¼
def verify_password(password, hashed_password):
    return bcrypt.checkpw(password.encode(), hashed_password.encode())

# secrets.tomlã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
def load_users_from_secrets():
    users = []
    try:
        for _, user in st.secrets["users"].items():
            # valueã¯è¾æ›¸ã¾ãŸã¯JSONæ–‡å­—åˆ—ã®å ´åˆãŒã‚ã‚‹ã®ã§å¯¾å¿œ
            if isinstance(user, str):
                user = json.loads(user)
            users.append(user)
    except Exception as e:
        st.error(get_localized_text(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", f"Failed to load user information: {e}"))
        st.stop()
    return users

# ãƒ­ã‚°ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ ã®è¡¨ç¤º
def login_form():
    st.subheader(get_localized_text("ãƒ­ã‚°ã‚¤ãƒ³", "Login"))

    user_data = load_users_from_secrets()

    with st.form("login_form"):
        username_input = st.text_input(get_localized_text("ãƒ¦ãƒ¼ã‚¶ãƒ¼å", "Username"))
        password_input = st.text_input(get_localized_text("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", "Password"), type="password")
        submitted = st.form_submit_button(get_localized_text("ãƒ­ã‚°ã‚¤ãƒ³", "Login"))

    if submitted:
        found_user = False
        for user in user_data:
            if username_input == user["username"]:
                found_user = True
                if verify_password(password_input, user["password_hash"]):
                    st.session_state["logged_in"] = True
                    st.session_state["username"] = user["username"]
                    st.success(get_localized_text("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸï¼", "Login successful!"))
                    st.rerun()
                else:
                    st.error(get_localized_text("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™ã€‚", "Incorrect password."))
                break
        if not found_user:
            st.error(get_localized_text("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", "User not found."))

# --- ã‚¢ãƒ—ãƒªæœ¬ä½“ã®ã‚¯ãƒ©ã‚¹ã¨é–¢æ•° (v1.0.0.py ã‹ã‚‰ã®ç§»è¡Œ) ---

class SessionManager:
    @staticmethod
    def initialize():
        session_vars = {
            'upload_files': [],
            'template_store': [],
            'analysis_log': [],
            'comparison_template': {},
            'current_data': None,
            'selected_teams': None, # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®çŠ¶æ…‹ã‚’ä¿æŒã™ã‚‹ãŸã‚ã«Noneã‚’è¨±å®¹
            'date_range': [None, None],
            'analysis_count': 0,
            'heatmap_count': 0,
            'trend_count': 0,
            'ranking_count': 0,
            'dfmain': None,
            'uploaded_file_processed': False,
            'num_uploaders': 1,
            'previous_files_hash': None
        }
        
        for var, default in session_vars.items():
            if var not in st.session_state:
                st.session_state[var] = default

class DataProcessor:
    @staticmethod
    def safe_read_csv(file):
        try:
            df = pd.read_csv(
                file,
                encoding='utf-8-sig',
                on_bad_lines='warn',
            )
            return df, None
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(
                    file,
                    encoding='cp932',
                    on_bad_lines='warn',
                )
                return df, None
            except Exception as e:
                return None, get_localized_text(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}", f"Read error: {str(e)}")
        except Exception as e:
            return None, get_localized_text(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}", f"Read error: {str(e)}")

    @staticmethod
    def process_dataframe(df):
        if df is None:
            return None
            
        if 'å®Ÿæ–½æ—¥' in df.columns:
            # å®Ÿæ–½æ—¥åˆ—ã®å¤‰æ›ã¨NaNãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 
            initial_nan_count = df['å®Ÿæ–½æ—¥'].isnull().sum()
            df['å®Ÿæ–½æ—¥'] = pd.to_datetime(df['å®Ÿæ–½æ—¥'], errors='coerce')
            # æ™‚é–“éƒ¨åˆ†ã‚’å‰Šé™¤ã—ã€æ—¥ä»˜ã®ã¿ã«ã™ã‚‹ï¼ˆ.dt.normalize()ã‚’è¿½åŠ ï¼‰
            df['å®Ÿæ–½æ—¥'] = df['å®Ÿæ–½æ—¥'].dt.normalize().dt.date 

            nan_after_coerce = df['å®Ÿæ–½æ—¥'].isnull().sum()
            
            if nan_after_coerce > initial_nan_count:
                newly_coerced_nan_percentage = (nan_after_coerce - initial_nan_count) / len(df) * 100
                if newly_coerced_nan_percentage > 10: # ä¾‹ãˆã°10%ä»¥ä¸Šã®å€¤ãŒç„¡åŠ¹ã«ãªã£ãŸå ´åˆã«è­¦å‘Š
                    st.warning(get_localized_text(
                        f"ã€Œå®Ÿæ–½æ—¥ã€åˆ—ã®{newly_coerced_nan_percentage:.1f}%ãŒæ—¥ä»˜ã¨ã—ã¦èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å…ƒã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                        f"{newly_coerced_nan_percentage:.1f}% of 'å®Ÿæ–½æ—¥' column could not be recognized as dates. Please check the original data format."
                    ))

        numeric_cols = ['ç”³è¾¼æ•°', 'å‚åŠ è€…æ•°', 'ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°', 'å®£ä¼å›æ•°', 'æº€è¶³å›ç­”']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce') 
        
        if all(col in df.columns for col in ['å‚åŠ è€…æ•°', 'ç”³è¾¼æ•°']):
            df['å‚åŠ ç‡(%)'] = (df['å‚åŠ è€…æ•°'] / df['ç”³è¾¼æ•°']) * 100
        if all(col in df.columns for col in ['æº€è¶³å›ç­”', 'å‚åŠ è€…æ•°']):
            df['æº€è¶³ç‡(%)'] = (df['æº€è¶³å›ç­”'] / df['å‚åŠ è€…æ•°']) * 100
        if all(col in df.columns for col in ['ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°', 'å‚åŠ è€…æ•°']):
            df['ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‡'] = df['ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°'] / df['å‚åŠ è€…æ•°']
        
        return df

    @staticmethod
    def expand_time_slots(df):
        if 'æ™‚é–“å¸¯' in df.columns:
            df['æ™‚é–“å¸¯'] = df['æ™‚é–“å¸¯'].astype(str)
            df['æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ'] = df['æ™‚é–“å¸¯'].str.split('ãƒ»')
            df = df.explode('æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ').reset_index(drop=True) # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚»ãƒƒãƒˆã‚’è¿½åŠ 
            df['æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ'] = df['æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ'].str.strip()
        return df

# ãƒ¡ã‚¤ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®è¡¨ç¤º (v1.0.0.py ã® main é–¢æ•°ã‚’ show_main_app ã«ãƒªãƒãƒ¼ãƒ )
def show_main_app():
    st.markdown("""
    <style>
    @font-face {
    font-family: 'Noto Sans JP';
    src: url('/static/NotoSansJP-Thin.woff2') format('woff2');
    }
    body, div, p, span, input, label {
    font-family: 'Noto Sans JP', sans-serif !important;
    }
    </style>
    """, unsafe_allow_html=True)

    st.title(get_localized_text("VRã‚¤ãƒ™ãƒ³ãƒˆåˆ†æã‚¢ãƒ—ãƒª", "VR Event Analysis App"))

    SessionManager.initialize()

    with st.sidebar:
        st.markdown(get_localized_text("## ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š", "## ğŸ” Filter Settings"))
        st.markdown("---")

        dfmain_for_sidebar = st.session_state.get('dfmain')

        if dfmain_for_sidebar is not None and not dfmain_for_sidebar.empty:
            df_filtered = dfmain_for_sidebar.copy()

            if 'æ‹…å½“ãƒãƒ¼ãƒ ' in df_filtered.columns:
                teams = sorted(df_filtered['æ‹…å½“ãƒãƒ¼ãƒ '].dropna().unique())
                
                # selected_teams ã®åˆæœŸå€¤ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°å…¨é¸æŠ
                initial_selected_teams = st.session_state.get('selected_teams')
                # selected_teams ãŒNoneã®å ´åˆã€å…¨ãƒãƒ¼ãƒ ã‚’é¸æŠçŠ¶æ…‹ã«ã™ã‚‹
                if initial_selected_teams is None:
                    initial_selected_teams = teams
                
                selected_teams = st.multiselect(
                    get_localized_text("ğŸ‘¥ æ‹…å½“ãƒãƒ¼ãƒ ", "ğŸ‘¥ Responsible Teams"), 
                    teams, 
                    default=[t for t in initial_selected_teams if t in teams] # å­˜åœ¨ã—ãªã„ãƒãƒ¼ãƒ ãŒdefaultã«å«ã¾ã‚Œãªã„ã‚ˆã†ã«ãƒ•ã‚£ãƒ«ã‚¿
                )
                st.session_state.selected_teams = selected_teams # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«é¸æŠã‚’ä¿å­˜

                # æ‹…å½“ãƒãƒ¼ãƒ ãŒä½•ã‚‚é¸æŠã•ã‚Œã¦ã„ãªã„å ´åˆã®å‹•ä½œå¤‰æ›´
                if len(selected_teams) == 0:
                    st.warning(get_localized_text("æ‹…å½“ãƒãƒ¼ãƒ ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…¨ã¦ã®æ‹…å½“ãƒãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚", "No teams selected. Data for all teams will be displayed."))
                    # df_filtered ã¯ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã«å…¥ã‚‹å‰ã®çŠ¶æ…‹ï¼ˆdfmain_for_sidebarï¼‰ã®ã¾ã¾ä½¿ç”¨
                else:
                    df_filtered = df_filtered[df_filtered['æ‹…å½“ãƒãƒ¼ãƒ '].isin(selected_teams)]

            if 'å®Ÿæ–½æ—¥' in df_filtered.columns:
                # dt.dateã«å¤‰æ›ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãã®ã¾ã¾ä½¿ç”¨
                valid_dates = df_filtered['å®Ÿæ–½æ—¥'].dropna()
                if not valid_dates.empty:
                    # Pythonã®dateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯min()/max()ã§ç›´æ¥æ¯”è¼ƒå¯èƒ½
                    min_date = min(valid_dates)
                    max_date = max(valid_dates)
                    
                    default_date_range = [min_date, max_date] if min_date <= max_date else [min_date, min_date]
                    date_range = st.date_input(get_localized_text("ğŸ“… å®Ÿæ–½æ—¥ã®ç¯„å›²", "ğŸ“… Date Range"), value=default_date_range)
                    
                    if isinstance(date_range, (list, tuple)) and len(date_range) == 2:
                        start, end = date_range[0], date_range[1]
                        df_filtered = df_filtered[
                            (df_filtered['å®Ÿæ–½æ—¥'] >= start) & 
                            (df_filtered['å®Ÿæ–½æ—¥'] <= end)
                        ]
                else:
                    st.warning(get_localized_text("æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "No date data available."))
            
            st.session_state.current_data = df_filtered

        else:
            st.info(get_localized_text("ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", "Please upload data."))

    tabs = st.tabs([
        get_localized_text("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç®¡ç†", "ğŸ“Š Data Management"),
        get_localized_text("ğŸ“ˆ åˆ†æãƒ»æ¯”è¼ƒ", "ğŸ“ˆ Analysis & Comparison"),
        get_localized_text("ğŸ“Š ã‚¯ãƒ­ã‚¹é›†è¨ˆ", "ğŸ“Š Cross Tabulation"),
        get_localized_text("ğŸ•’ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—", "ğŸ•’ Heatmap"),
        get_localized_text("ğŸ“‰ æ™‚ç³»åˆ—", "ğŸ“‰ Time Series"),
        get_localized_text("ğŸ† ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ğŸ† Ranking"),
        get_localized_text("ğŸ“‹ è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆ", "ğŸ“‹ Automatic Report")
    ])

    with tabs[0]:
        st.header(get_localized_text("ğŸ“ åˆ†æå¯¾è±¡CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "ğŸ“ Upload CSV File for Analysis"))
        
        col1, col2 = st.columns([2, 1])
        with col1:
            all_uploaded_files_current_run = [] 
            uploader_objects = [] 
            last_uploaded_idx = -1 

            for i in range(st.session_state.num_uploaders):
                label = get_localized_text("CSVãƒ•ã‚¡ã‚¤ãƒ« (å¿…é ˆ)", "CSV File (Required)") if i == 0 else get_localized_text(f"è¿½åŠ ã®CSVãƒ•ã‚¡ã‚¤ãƒ« (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ {i})", f"Additional CSV File (Optional {i})")
                file_obj = st.file_uploader(
                    label,
                    type="csv",
                    key=f"csv_upload_{i}"
                )
                uploader_objects.append(file_obj)
                
                if file_obj is not None:
                    last_uploaded_idx = i
                    all_uploaded_files_current_run.append(file_obj)

            new_num_uploaders = st.session_state.num_uploaders

            if last_uploaded_idx != -1:
                target_num_uploaders = last_uploaded_idx + 2
                new_num_uploaders = max(new_num_uploaders, target_num_uploaders)
            else:
                new_num_uploaders = 1
            
            if last_uploaded_idx != -1 and st.session_state.num_uploaders > (last_uploaded_idx + 2):
                new_num_uploaders = last_uploaded_idx + 2
            elif last_uploaded_idx == -1 and st.session_state.num_uploaders > 1:
                new_num_uploaders = 1

            if new_num_uploaders != st.session_state.num_uploaders:
                st.session_state.num_uploaders = new_num_uploaders
                st.rerun()

            current_files_hash = hash(tuple((f.name, f.size) for f in all_uploaded_files_current_run if f is not None))
            previous_files_hash = st.session_state.get('previous_files_hash', None)

            files_changed = (current_files_hash != previous_files_hash)
            
            if files_changed and all_uploaded_files_current_run:
                st.session_state.upload_files = all_uploaded_files_current_run
                st.session_state.uploaded_file_processed = False
                st.session_state.previous_files_hash = current_files_hash
                st.rerun()

            if st.session_state.upload_files and not st.session_state.uploaded_file_processed:
                uploaded_dfs_temp = []
                for f in st.session_state.upload_files:
                    f.seek(0)
                    df_t, error = DataProcessor.safe_read_csv(f)
                    if error:
                        st.error(get_localized_text(f"ãƒ•ã‚¡ã‚¤ãƒ« {f.name}: {error}", f"File {f.name}: {error}"))
                    else:
                        df_t = DataProcessor.process_dataframe(df_t)
                        if df_t is not None:
                            uploaded_dfs_temp.append(df_t)

                if uploaded_dfs_temp:
                    df_combined = pd.concat(uploaded_dfs_temp, ignore_index=True).drop_duplicates()
                    df_combined = DataProcessor.expand_time_slots(df_combined)
                    st.session_state['dfmain'] = df_combined
                    st.session_state.current_data = df_combined
                    st.session_state.uploaded_file_processed = True

                    numeric_cols_for_check = ['ç”³è¾¼æ•°', 'å‚åŠ è€…æ•°', 'ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°', 'å®£ä¼å›æ•°', 'æº€è¶³å›ç­”']
                    high_nan_cols = []
                    for col in numeric_cols_for_check:
                        if col in df_combined.columns:
                            nan_percentage = df_combined[col].isnull().sum() / len(df_combined) * 100
                            if nan_percentage > 50: 
                                high_nan_cols.append(f"{col} ({nan_percentage:.1f}%)")
                    if high_nan_cols:
                        st.warning(get_localized_text(
                            f"ä»¥ä¸‹ã®æ•°å€¤åˆ—ã§é«˜ã„æ¬ æç‡ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {', '.join(high_nan_cols)}ã€‚\n"
                            "ã“ã‚Œã¯ã€å…ƒã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã®è©²å½“åˆ—ã«æ•°å€¤ä»¥å¤–ã®ãƒ‡ãƒ¼ã‚¿ãŒå¤šãå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
                            "ãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºæ€§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                            f"High missing values detected in the following numeric columns: {', '.join(high_nan_cols)}.\n"
                            "This may indicate that the original CSV file contains non-numeric data in these columns. Please verify data accuracy."
                        ))
                else:
                    st.session_state['dfmain'] = None
                    st.session_state.current_data = None
                    st.session_state.uploaded_file_processed = True
            
        df_display = st.session_state.get('current_data')
        if df_display is None or df_display.empty:
            st.warning(get_localized_text("ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "No data uploaded or no data after filtering."))
        else:
            if st.session_state.upload_files:
                with col2:
                    st.markdown(get_localized_text("### ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±", "### ğŸ“Š File Information"))
                    for i, f in enumerate(all_uploaded_files_current_run, 1):
                        if f is not None:
                            display_name = f.name if hasattr(f, 'name') else get_localized_text(f"ãƒ•ã‚¡ã‚¤ãƒ« {i}", f"File {i}")
                            st.info(get_localized_text(f"ãƒ•ã‚¡ã‚¤ãƒ« {i}: {display_name}", f"File {i}: {display_name}"))

                if len(st.session_state.upload_files) > 1: 
                    st.markdown(get_localized_text("### ğŸ”„ ãƒ‡ãƒ¼ã‚¿çµ±åˆçµæœ", "### ğŸ”„ Data Integration Result"))
                    total_rows_before = 0
                    for f in st.session_state.upload_files:
                        f.seek(0)
                        temp_df, _ = DataProcessor.safe_read_csv(f)
                        if temp_df is not None:
                            total_rows_before += len(temp_df)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric(get_localized_text("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿æ•°", "Total Uploaded Rows"), f"{total_rows_before}{get_localized_text('è¡Œ', ' rows')}")
                    with col2:
                        st.metric(get_localized_text("çµ±åˆå¾Œã®ãƒ‡ãƒ¼ã‚¿æ•°", "Total Integrated Rows"), f"{len(df_display)}{get_localized_text('è¡Œ', ' rows')}")
                    with col3:
                        removed_rows = total_rows_before - len(df_display)
                        if removed_rows > 0:
                            st.metric(get_localized_text("é‡è¤‡å‰Šé™¤", "Duplicates Removed"), f"{removed_rows}{get_localized_text('è¡Œ', ' rows')}")
                            st.info(get_localized_text(
                                f"â€» {removed_rows}è¡Œã®é‡è¤‡ï¼ˆå…¨ã¦ã®åˆ—ãŒå®Œå…¨ã«ä¸€è‡´ã™ã‚‹è¡Œï¼‰ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚",
                                f"â€» {removed_rows} duplicate rows (all columns perfectly matching) were removed."
                            ))
                else:
                    st.info(get_localized_text(f"å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{len(df_display)}è¡Œï¼‰ã‚’å‡¦ç†ã—ã¾ã™", f"Processing single file ({len(df_display)} rows)"))

            st.markdown(get_localized_text("### ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", "### ğŸ“‹ Data Preview"))
            preview_rows = st.number_input(
                get_localized_text("è¡¨ç¤ºã™ã‚‹è¡Œæ•°ï¼ˆ0ã§å…¨è¡Œè¡¨ç¤ºï¼‰", "Number of rows to display (0 for all)"),
                min_value=0,
                max_value=len(df_display),
                value=5
            )
            
            if preview_rows == 0:
                st.dataframe(df_display, use_container_width=True)
            else:
                st.dataframe(df_display.head(preview_rows), use_container_width=True)

            st.markdown(get_localized_text("### ğŸ“‹ åˆ—æƒ…å ±", "### ğŸ“‹ Column Information"))
            col_info = pd.DataFrame({
                get_localized_text('ãƒ‡ãƒ¼ã‚¿å‹', 'Data Type'): df_display.dtypes,
                get_localized_text('æ¬ æå€¤æ•°', 'Missing Count'): df_display.isnull().sum(),
                get_localized_text('æ¬ æç‡(%)', 'Missing Rate (%)'): (df_display.isnull().sum() / len(df_display) * 100).round(2),
                get_localized_text('ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°', 'Unique Count'): df_display.nunique(),
            })
            st.dataframe(col_info)

            st.markdown(get_localized_text("### ğŸ“Š åŸºæœ¬çµ±è¨ˆé‡", "### ğŸ“Š Basic Statistics"))
            numeric_cols = df_display.select_dtypes(include=['int64', 'float64']).columns
            if not numeric_cols.empty:
                stats_df = df_display[numeric_cols].describe().T

                rename_map = {
                    "count": get_localized_text("ãƒ‡ãƒ¼ã‚¿æ•°", "Count"),
                    "mean": get_localized_text("å¹³å‡", "Mean"),
                    "std": get_localized_text("æ¨™æº–åå·®", "Std Dev"),
                    "min": get_localized_text("æœ€å°å€¤", "Min"),
                    "25%": "25%",
                    "50%": get_localized_text("ä¸­å¤®å€¤", "Median"),
                    "75%": "75%",
                    "max": get_localized_text("æœ€å¤§å€¤", "Max")
                }

                stats_df = stats_df.rename(columns=rename_map)
                st.dataframe(stats_df)


            st.markdown(get_localized_text("### ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ—ã®å†…è¨³", "### ğŸ“Š Category Column Breakdown"))
            category_cols = df_display.select_dtypes(include=['object']).columns
            if not category_cols.empty:
                selected_col = st.selectbox(get_localized_text("ç¢ºèªã™ã‚‹åˆ—ã‚’é¸æŠ", "Select column to check"), category_cols)
                value_counts = df_display[selected_col].value_counts()
                
                fig, ax = plt.subplots(figsize=(10, 5))
                value_counts.plot(kind='bar', ax=ax)
                
                title_font = {'fontfamily': plt.rcParams['font.family'], 'fontsize': 12}
                ax.set_title(get_graph_text(f"{selected_col}ã®å€¤ã‚«ã‚¦ãƒ³ãƒˆ", f"Value Counts of {selected_col}", fontproperties=font_prop), fontproperties=font_prop, **title_font)
                ax.set_xlabel(get_graph_text(selected_col, selected_col, fontproperties=font_prop), fontproperties=font_prop)
                ax.set_ylabel(get_graph_text("ã‚«ã‚¦ãƒ³ãƒˆ", "Count", fontproperties=font_prop), fontproperties=font_prop)
                
                for label in ax.get_xticklabels():
                    label.set_fontfamily(plt.rcParams['font.family'])
                for label in ax.get_yticklabels():
                    label.set_fontfamily(plt.rcParams['font.family'])
                
                plt.xticks(rotation=45)
                plt.tight_layout()
                st.pyplot(fig)

    with tabs[1]:
        st.header(get_localized_text("ğŸ“ˆ åˆ†æãƒ»æ¯”è¼ƒ", "ğŸ“ˆ Analysis & Comparison"))
        
        if 'current_data' in st.session_state and st.session_state.current_data is not None and not st.session_state.current_data.empty:
            df = st.session_state.current_data.copy()
            
            col1, col2 = st.columns(2)
            with col1:
                num_cols = df.select_dtypes(include='number').columns.tolist()
                cat_cols = df.select_dtypes(include='object').columns.tolist()
                
                if not num_cols:
                    st.warning(get_localized_text("æ•°å€¤åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "No numeric columns found."))
                    st.stop()
                if not cat_cols:
                    st.warning(get_localized_text("ã‚«ãƒ†ã‚´ãƒªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "No categorical columns found."))
                    st.stop()

                target_num = st.selectbox(get_localized_text("åˆ†æå¯¾è±¡ï¼ˆæ•°å€¤ï¼‰", "Analysis Target (Numeric)"), num_cols)
                group_col = st.selectbox(get_localized_text("ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰", "Group By (Category)"), cat_cols)

            with col2:
                agg_options = [
                    get_localized_text('å¹³å‡', 'Mean'),
                    get_localized_text('åˆè¨ˆ', 'Sum'),
                    get_localized_text('ä¸­å¤®å€¤', 'Median'),
                    get_localized_text('æœ€å¤§', 'Max'),
                    get_localized_text('æœ€å°', 'Min'),
                    get_localized_text('ãƒ‡ãƒ¼ã‚¿æ•°', 'Count')
                ]
                selected_aggs_display = st.multiselect(
                    get_localized_text("è¡¨ç¤ºã™ã‚‹çµ±è¨ˆæŒ‡æ¨™", "Select Statistical Metrics to Display"),
                    agg_options,
                    default=agg_options
                )
                exclude_outliers = st.checkbox(get_localized_text("å¤–ã‚Œå€¤ã‚’é™¤å¤–", "Exclude Outliers"))

            try:
                analysis_df = df.copy()
                
                if exclude_outliers and target_num in analysis_df.columns and analysis_df[target_num].std() > 0:
                    z_scores = np.abs((analysis_df[target_num] - analysis_df[target_num].mean()) / 
                                    analysis_df[target_num].std())
                    analysis_df = analysis_df[z_scores < 3]
                elif exclude_outliers: # std=0ã®å ´åˆ
                    st.info(get_localized_text(
                        f"'{target_num}'ã®ãƒ‡ãƒ¼ã‚¿ã«ã°ã‚‰ã¤ããŒãªã„ãŸã‚ã€å¤–ã‚Œå€¤é™¤å¤–ã¯é©ç”¨ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚",
                        f"Outlier exclusion was not applied as '{target_num}' data has no variance."
                    ))


                agg_map_internal = {
                    get_localized_text('å¹³å‡', 'Mean'): 'mean',
                    get_localized_text('åˆè¨ˆ', 'Sum'): 'sum',
                    get_localized_text('ä¸­å¤®å€¤', 'Median'): 'median',
                    get_localized_text('æœ€å¤§', 'Max'): 'max',
                    get_localized_text('æœ€å°', 'Min'): 'min',
                    get_localized_text('ãƒ‡ãƒ¼ã‚¿æ•°', 'Count'): 'count'
                }
                
                agg_funcs_list = [agg_map_internal[a] for a in selected_aggs_display if a in agg_map_internal] 

                if group_col in analysis_df.columns and target_num in analysis_df.columns:
                    grouped_df = analysis_df.groupby(group_col)[target_num].agg(agg_funcs_list)
                    
                    # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ãƒªãƒãƒ¼ãƒ 
                    rename_dict = {agg_map_internal[a]: f"{target_num}_{a}" for a in selected_aggs_display if agg_map_internal[a] in grouped_df.columns}
                    grouped_df.rename(columns=rename_dict, inplace=True)

                else:
                    st.error(get_localized_text("é¸æŠã•ã‚ŒãŸåˆ—ãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚", "Selected columns do not exist in the dataframe."))
                    grouped_df = pd.DataFrame()

                if not grouped_df.empty:
                    st.markdown(get_localized_text("### ğŸ“Š ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥é›†è¨ˆçµæœ", "### ğŸ“Š Grouped Aggregation Results"))
                    st.dataframe(grouped_df.round(2), use_container_width=True)

                    if selected_aggs_display:
                        st.markdown(get_localized_text("### ğŸ“ˆ çµ±è¨ˆæŒ‡æ¨™åˆ¥ã‚°ãƒ©ãƒ•", "### ğŸ“ˆ Graphs by Statistical Metric"))
                        cols = st.columns(2)
                        for i, metric_display_name in enumerate(selected_aggs_display):
                            col_name_for_plot = f"{target_num}_{metric_display_name}" 
                            if col_name_for_plot in grouped_df.columns:
                                with cols[i % 2]:
                                    fig, ax = plt.subplots(figsize=(8, 4))
                                    grouped_df[col_name_for_plot].plot(kind='bar', ax=ax)
                                    ax.set_ylabel(get_graph_text(f"{target_num}ã®{metric_display_name}", f"{metric_display_name} of {target_num}", fontproperties=font_prop), fontfamily=plt.rcParams['font.family'])
                                    ax.set_title(get_graph_text(f"{group_col}ã”ã¨ã®{target_num}ï¼ˆ{metric_display_name}ï¼‰", f"{metric_display_name} of {target_num} by {group_col}", fontproperties=font_prop), fontfamily=plt.rcParams['font.family'])
                                    plt.xticks(rotation=45)
                                    plt.tight_layout()
                                    st.pyplot(fig)

                                    buf = io.BytesIO()
                                    fig.savefig(buf, format='png')
                                    st.download_button(
                                        get_localized_text(f"ğŸ“¥ {metric_display_name}ã®ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜", f"ğŸ“¥ Save {metric_display_name} Graph"),
                                        buf.getvalue(),
                                        f"analysis_{metric_display_name}.png",
                                        "image/png"
                                    )
                else:
                    st.warning(get_localized_text("é›†è¨ˆçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚", "No aggregation results. Please check filter settings or data."))

            except Exception as e:
                st.error(get_localized_text(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}", f"Analysis error: {e}"))
        else:
            st.warning(get_localized_text("ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¿ãƒ–ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "No data uploaded or no data after filtering. Please upload files in the Data Management tab."))

    with tabs[2]:
        st.header(get_localized_text("ğŸ“Š ã‚¯ãƒ­ã‚¹é›†è¨ˆ", "ğŸ“Š Cross Tabulation"))

        if 'current_data' in st.session_state and st.session_state.current_data is not None and not st.session_state.current_data.empty:
            df = st.session_state.current_data.copy()

            cat_cols = df.select_dtypes(include='object').columns.tolist()
            num_cols = df.select_dtypes(include='number').columns.tolist()

            if len(cat_cols) < 2:
                st.warning(get_localized_text("ã‚¯ãƒ­ã‚¹é›†è¨ˆã«ã¯2ã¤ä»¥ä¸Šã®ã‚«ãƒ†ã‚´ãƒªåˆ—ãŒå¿…è¦ã§ã™ã€‚", "Cross tabulation requires at least two categorical columns."))
                st.stop()
            if not num_cols:
                st.warning(get_localized_text("æ•°å€¤åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "No numeric columns found."))
                st.stop()

            col1 = st.selectbox(get_localized_text("è¡Œã‚«ãƒ†ã‚´ãƒª", "Row Category"), cat_cols, key="cross_row")
            col2_options = [c for c in cat_cols if c != col1]
            if not col2_options:
                st.warning(get_localized_text(f"'{col1}' ä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", f"No categorical columns other than '{col1}' found."))
                st.stop()
            col2 = st.selectbox(get_localized_text("åˆ—ã‚«ãƒ†ã‚´ãƒª", "Column Category"), col2_options, key="cross_col")
            
            num_col = st.selectbox(get_localized_text("æ•°å€¤é …ç›®", "Numeric Item"), num_cols, key="cross_num")
            agg_method_display = st.selectbox(get_localized_text("é›†è¨ˆæ–¹æ³•", "Aggregation Method"), [
                get_localized_text('å¹³å‡', 'Mean'),
                get_localized_text('åˆè¨ˆ', 'Sum'),
                get_localized_text('ä¸­å¤®å€¤', 'Median'),
                get_localized_text('æœ€å¤§', 'Max'),
                get_localized_text('æœ€å°', 'Min'),
                get_localized_text('ãƒ‡ãƒ¼ã‚¿æ•°', 'Count')
            ], key="cross_agg")

            if st.button(get_localized_text("ã‚¯ãƒ­ã‚¹é›†è¨ˆã‚’å®Ÿè¡Œ", "Execute Cross Tabulation"), key="cross_execute"):
                try:
                    agg_map_internal = {
                        get_localized_text('å¹³å‡', 'Mean'): 'mean',
                        get_localized_text('åˆè¨ˆ', 'Sum'): 'sum',
                        get_localized_text('ä¸­å¤®å€¤', 'Median'): 'median',
                        get_localized_text('æœ€å¤§', 'Max'): 'max',
                        get_localized_text('æœ€å°', 'Min'): 'min',
                        get_localized_text('ãƒ‡ãƒ¼ã‚¿æ•°', 'Count'): 'count'
                    }

                    if col1 in df.columns and col2 in df.columns and num_col in df.columns:
                        cross_table = pd.pivot_table(
                            df,
                            values=num_col,
                            index=col1,
                            columns=col2,
                            aggfunc=agg_map_internal[agg_method_display]
                        )
                        if isinstance(cross_table.columns, pd.MultiIndex):
                            cross_table.columns = cross_table.columns.droplevel(0)

                        st.dataframe(cross_table)

                        fig, ax = plt.subplots(figsize=(10, 5))
                        cross_table.plot(kind='bar', ax=ax)
                        ax.set_ylabel(get_graph_text(num_col, num_col, fontproperties=font_prop), fontfamily=plt.rcParams['font.family']) # Column name is fine
                        ax.set_title(get_graph_text(f"{col1} Ã— {col2} ã® {agg_method_display}", f"{agg_method_display} of {num_col} by {col1} x {col2}", fontproperties=font_prop), fontfamily=plt.rcParams['font.family'])
                        plt.xticks(rotation=45)
                        plt.tight_layout()
                        st.pyplot(fig)

                        csv = cross_table.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                        st.download_button(
                            get_localized_text("ğŸ“¥ ã‚¯ãƒ­ã‚¹é›†è¨ˆçµæœã‚’CSVã§ä¿å­˜", "ğŸ“¥ Save Cross Tabulation Result as CSV"),
                            csv,
                            file_name="cross_table.csv",
                            mime='text/csv',
                            key="cross_download"
                        )
                    else:
                        st.error(get_localized_text("é¸æŠã•ã‚ŒãŸåˆ—ãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚", "Selected columns do not exist in the dataframe."))

                except Exception as e:
                    st.error(get_localized_text(f"ã‚¯ãƒ­ã‚¹é›†è¨ˆã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", f"Failed to execute cross tabulation: {e}"))
        else:
            st.warning(get_localized_text("ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¿ãƒ–ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "No data uploaded or no data after filtering. Please upload files in the Data Management tab."))


    with tabs[3]:
        st.header(get_localized_text("ğŸŸ¥ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—", "ğŸŸ¥ Heatmap"))

        if 'current_data' in st.session_state and st.session_state.current_data is not None and not st.session_state.current_data.empty:
            df = st.session_state.current_data.copy()

            df = DataProcessor.expand_time_slots(df)

            if 'æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ' not in df.columns or 'æ›œæ—¥' not in df.columns:
                st.warning(get_localized_text("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€Œæ™‚é–“å¸¯ã€ã¨ã€Œæ›œæ—¥ã€ã®åˆ—ãŒå¿…è¦ã§ã™ã€‚", "Time slot and weekday columns are required to create a heatmap."))
                st.stop()
            
            if not all(col in df.columns for col in ['æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ', 'æ›œæ—¥']):
                st.warning(get_localized_text("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä½œæˆã«å¿…è¦ãªåˆ—ï¼ˆæ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆã€æ›œæ—¥ï¼‰ãŒãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚", "Required columns for heatmap (Time Slot, Weekday) do not exist in the data."))
                st.stop()


            numeric_cols = df.select_dtypes(include='number').columns.tolist()
            if not numeric_cols:
                st.warning(get_localized_text("æ•°å€¤åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¯æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚", "No numeric columns found. Heatmap is based on numeric data."))
                st.stop()

            col1, col2 = st.columns(2)
            with col1:
                heat_metric = st.selectbox(
                    get_localized_text("é›†è¨ˆã™ã‚‹æŒ‡æ¨™", "Metric to Aggregate"),
                    numeric_cols,
                    key="heat_metric"
                )
                agg_method_display = st.selectbox(
                    get_localized_text("é›†è¨ˆæ–¹æ³•", "Aggregation Method"),
                    [get_localized_text('å¹³å‡', 'Mean'), get_localized_text('åˆè¨ˆ', 'Sum'), get_localized_text('ãƒ‡ãƒ¼ã‚¿æ•°', 'Count')],
                    key="heat_agg"
                )

            with col2:
                color_scale = st.selectbox(
                    get_localized_text("ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«", "Color Scale"),
                    ['YlOrRd', 'viridis', 'coolwarm'],
                    key="heat_color"
                )
                normalize = st.checkbox(get_localized_text("ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–", "Normalize Data"), value=True, key="heat_normalize")

            if st.button(get_localized_text("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ", "Generate Heatmap"), key="heat_execute"):
                try:
                    heat_df = df.copy()

                    agg_map_internal = {
                        get_localized_text('å¹³å‡', 'Mean'): 'mean',
                        get_localized_text('åˆè¨ˆ', 'Sum'): 'sum',
                        get_localized_text('ãƒ‡ãƒ¼ã‚¿æ•°', 'Count'): 'count'
                    }
                    pivot_table = pd.pivot_table(
                        heat_df,
                        values=heat_metric,
                        index='æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ',
                        columns='æ›œæ—¥',
                        aggfunc=agg_map_internal[agg_method_display]
                        )

                    weekdays = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
                    existing_weekdays = [day for day in weekdays if day in pivot_table.columns]
                    pivot_table = pivot_table.reindex(columns=existing_weekdays)
                    
                    try:
                        pivot_table = pivot_table.reindex(sorted(pivot_table.index))
                    except Exception:
                        pass

                    if normalize and not pivot_table.empty and pivot_table.std().std() > 0:
                        pivot_table = (pivot_table - pivot_table.mean().mean()) / pivot_table.std().std()
                    elif normalize and not pivot_table.empty: # æ¨™æº–åå·®ãŒ0ã®å ´åˆ
                        st.info(get_localized_text("ãƒ‡ãƒ¼ã‚¿ã«ã°ã‚‰ã¤ããŒãªã„ãŸã‚ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®æ­£è¦åŒ–ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚", "Heatmap normalization skipped as data has no variance."))


                    fig, ax = plt.subplots(figsize=(12, 8))
                    sns.heatmap(
                        pivot_table,
                        cmap=color_scale,
                        annot=True,
                        fmt='.2f',
                        linewidths=.5,
                        linecolor='black',
                        ax=ax
                    )

                    ax.set_title(get_graph_text(f"æ™‚é–“å¸¯Ã—æ›œæ—¥ã®{heat_metric}ï¼ˆ{agg_method_display}ï¼‰", f"{agg_method_display} of {heat_metric} by Time Slot x Weekday", fontproperties=font_prop), fontfamily=plt.rcParams['font.family'], fontsize=16)
                    ax.set_xlabel(get_graph_text("æ›œæ—¥", "Weekday", fontproperties=font_prop), fontfamily=plt.rcParams['font.family'], fontsize=12)
                    ax.set_ylabel(get_graph_text("æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ", "Time Slot", fontproperties=font_prop), fontfamily=plt.rcParams['font.family'], fontsize=12)

                    for label in ax.get_xticklabels():
                        label.set_fontfamily(plt.rcParams['font.family'])
                    for label in ax.get_yticklabels():
                        label.set_fontfamily(plt.rcParams['font.family'])

                    plt.xticks(rotation=45)
                    plt.tight_layout()
                    st.pyplot(fig)

                    csv = pivot_table.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button(
                        get_localized_text("ğŸ“¥ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ä¿å­˜", "ğŸ“¥ Save Heatmap Data as CSV"),
                        csv,
                        file_name="heatmap_data.csv",
                        mime='text/csv',
                        key="heat_download"
                    )

                    st.subheader(get_localized_text("ğŸ“Š ç‰¹å¾´çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³", "ğŸ“Š Characteristic Patterns"))

                    if not pivot_table.empty:
                        max_val_series = pivot_table.max()
                        if not max_val_series.empty:
                            max_val = max_val_series.max()
                        else:
                            max_val = np.nan

                        min_val_series = pivot_table.min()
                        if not min_val_series.empty:
                            min_val = min_val_series.min()
                        else:
                            min_val = np.nan

                        if not np.isnan(max_val) and not pivot_table[pivot_table == max_val].empty:
                            max_pos_df = pivot_table[pivot_table == max_val].stack().index[0]
                            st.info(get_localized_text(
                                f"ğŸ”º æœ€ã‚‚{heat_metric}ãŒé«˜ã„æ™‚é–“å¸¯: {max_pos_df[0]}ã®{max_pos_df[1]}æ›œæ—¥ ({max_val:.2f})",
                                f"ğŸ”º Highest {heat_metric} time: {max_pos_df[0]} on {max_pos_df[1]} ({max_val:.2f})"
                            ))
                        else:
                            st.info(get_localized_text("ğŸ”º æœ€ã‚‚é«˜ã„å€¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", "ğŸ”º Could not identify the highest value pattern."))

                        if not np.isnan(min_val) and not pivot_table[pivot_table == min_val].empty:
                            min_pos_df = pivot_table[pivot_table == min_val].stack().index[0]
                            st.info(get_localized_text(
                                f"ğŸ”» æœ€ã‚‚{heat_metric}ãŒä½ã„æ™‚é–“å¸¯: {min_pos_df[0]}ã®{min_pos_df[1]}æ›œæ—¥ ({min_val:.2f})",
                                f"ğŸ”» Lowest {heat_metric} time: {min_pos_df[0]} on {min_pos_df[1]} ({min_val:.2f})"
                            ))
                        else:
                            st.info(get_localized_text("ğŸ”» æœ€ã‚‚ä½ã„å€¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", "ğŸ”» Could not identify the lowest value pattern."))
                    else:
                        st.info(get_localized_text("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ç‰¹å¾´çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã€‚", "Heatmap data is empty, cannot identify characteristic patterns."))


                except Exception as e:
                    st.error(get_localized_text(f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", f"An error occurred during heatmap generation: {e}"))
        else:
            st.warning(get_localized_text("ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¿ãƒ–ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "No data uploaded or no data after filtering. Please upload files in the Data Management tab."))


    with tabs[4]:
        st.header(get_localized_text("ğŸ“‰ æ™‚ç³»åˆ—åˆ†æ", "ğŸ“‰ Time Series Analysis"))

        if 'current_data' in st.session_state and st.session_state.current_data is not None and not st.session_state.current_data.empty:
            df = st.session_state.current_data.copy()

            if 'å®Ÿæ–½æ—¥' not in df.columns:
                st.warning(get_localized_text("æ™‚ç³»åˆ—åˆ†æã«ã¯ã€Œå®Ÿæ–½æ—¥ã€ã®åˆ—ãŒå¿…è¦ã§ã™ã€‚", "The 'å®Ÿæ–½æ—¥' (Date) column is required for time series analysis."))
                st.stop()
            
            # å®Ÿæ–½æ—¥ãŒPythonã®dateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ãªã£ã¦ã„ã‚‹ãŸã‚ã€ãã®ã¾ã¾dropna
            df = df.dropna(subset=['å®Ÿæ–½æ—¥'])

            if df.empty:
                st.warning(get_localized_text("æœ‰åŠ¹ãªæ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "No valid date data available."))
                st.stop()

            # DatetimeIndexã®ä½œæˆç”¨ã«ã€dateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’Timestampã«å¤‰æ›
            df['å®Ÿæ–½æ—¥_timestamp'] = pd.to_datetime(df['å®Ÿæ–½æ—¥'])


            cat_cols = df.select_dtypes(include='object').columns.tolist()
            numeric_cols = df.select_dtypes(include='number').columns.tolist()

            if not numeric_cols:
                st.warning(get_localized_text("æ•°å€¤åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ™‚ç³»åˆ—åˆ†æã¯æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚", "No numeric columns found. Time series analysis is based on numeric data."))
                st.stop()

            col1, col2 = st.columns(2)
            with col1:
                trend_metric = st.selectbox(
                    get_localized_text("åˆ†æã™ã‚‹æŒ‡æ¨™", "Metric to Analyze"),
                    numeric_cols,
                    key="trend_metric"
                )
                trend_group = st.selectbox(
                    get_localized_text("ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰", "Group By (Optional)"),
                    ['ãªã—'] + cat_cols,
                    key="trend_group"
                )

            with col2:
                agg_period_display = st.selectbox(
                    get_localized_text("é›†è¨ˆæœŸé–“", "Aggregation Period"),
                    [get_localized_text('æ—¥æ¬¡', 'Daily'), get_localized_text('é€±æ¬¡', 'Weekly'), get_localized_text('æœˆæ¬¡', 'Monthly')],
                    key="trend_period"
                )
                moving_avg = st.number_input(
                    get_localized_text("ç§»å‹•å¹³å‡æœŸé–“", "Moving Average Period"), 
                    min_value=1,
                    max_value=30,
                    value=7,
                    key="trend_ma"
                )

            try:
                trend_df = df.copy()
                
                period_map_internal = {
                    get_localized_text('æ—¥æ¬¡', 'Daily'): 'D',
                    get_localized_text('é€±æ¬¡', 'Weekly'): 'W',
                    get_localized_text('æœˆæ¬¡', 'Monthly'): 'M'
                }

                fig, ax = plt.subplots(figsize=(12, 6))
                has_data_to_plot = False 

                if trend_group == 'ãªã—':
                    resampled = trend_df.set_index('å®Ÿæ–½æ—¥_timestamp')[trend_metric].resample(period_map_internal[agg_period_display]).mean()
                    
                    if not resampled.empty:
                        resampled.plot(ax=ax, label=get_graph_text(f'{agg_period_display}å¹³å‡', f'{agg_period_display} Average'))
                        moving = resampled.rolling(window=moving_avg, min_periods=1).mean() 
                        if not moving.empty:
                            moving.plot(ax=ax, label=get_graph_text(f'{moving_avg}{agg_period_display[0]}ç§»å‹•å¹³å‡', f'{moving_avg}{agg_period_display[0]} Moving Average'), style='--') 
                        else:
                            st.info(get_localized_text("ç§»å‹•å¹³å‡ã‚’è¨ˆç®—ã™ã‚‹ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "Insufficient data to calculate moving average."))
                        has_data_to_plot = True
                    else:
                        st.warning(get_localized_text("é›†è¨ˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", "Time series graph could not be generated as aggregation data is empty."))
                    
                    if not resampled.empty:
                        st.subheader(get_localized_text("ğŸ“Š æ™‚ç³»åˆ—ã®ç‰¹å¾´", "ğŸ“Š Time Series Characteristics"))
                        latest_val = resampled.iloc[-1]
                        prev_val = resampled.iloc[-2] if len(resampled) > 1 else None

                        st.info(get_localized_text(f"æœ€æ–°ã®{agg_period_display}å¹³å‡: {latest_val:.2f}", f"Latest {agg_period_display} average: {latest_val:.2f}"))
                        if prev_val is not None and prev_val != 0:
                            change = ((latest_val - prev_val) / prev_val * 100)
                            st.info(get_localized_text(f"ç›´è¿‘ã®{agg_period_display}ã‹ã‚‰ã®å¤‰åŒ–ç‡: {change:.1f}%", f"Change from previous {agg_period_display}: {change:.1f}%"))
                        else:
                            st.info(get_localized_text("ç›´è¿‘ã®å¤‰åŒ–ç‡ã‚’è¨ˆç®—ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "No data to calculate recent change rate."))
                    else:
                        st.info(get_localized_text("é›†è¨ˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ç‰¹å¾´ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã€‚", "Aggregation data is empty, cannot extract characteristics."))

                    if not resampled.empty:
                        csv = resampled.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                        st.download_button(
                            get_localized_text("ğŸ“¥ æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ä¿å­˜", "ğŸ“¥ Save Time Series Data as CSV"),
                            csv,
                            file_name="trend_data.csv",
                            mime='text/csv',
                            key="trend_download"
                        )

                else:
                    if trend_group in trend_df.columns:
                        groups = trend_df[trend_group].dropna().unique()
                        if len(groups) > 0:
                            for group in groups:
                                if pd.notna(group) and str(group).strip() != '':
                                    group_data = trend_df[trend_df[trend_group] == group]
                                    if not group_data.empty:
                                        resampled = group_data.set_index('å®Ÿæ–½æ—¥_timestamp')[trend_metric].resample(period_map_internal[agg_period_display]).mean()
                                        if not resampled.empty:
                                            resampled.plot(ax=ax, label=str(group)) # Group name is data, keep as is
                                            moving = resampled.rolling(window=moving_avg, min_periods=1).mean()
                                            if not moving.empty:
                                                moving.plot(ax=ax, label=get_graph_text(f'{str(group)} ({moving_avg}{agg_period_display[0]}ç§»å‹•å¹³å‡)', f'{str(group)} ({moving_avg}{agg_period_display[0]} Moving Average)'), style='--') 
                                            else:
                                                st.info(get_localized_text(f"ã‚°ãƒ«ãƒ¼ãƒ— '{group}' ã®ç§»å‹•å¹³å‡ã‚’è¨ˆç®—ã™ã‚‹ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", f"Insufficient data to calculate moving average for group '{group}'."))
                                            has_data_to_plot = True
                                        else:
                                            st.warning(get_localized_text(f"ã‚°ãƒ«ãƒ¼ãƒ— '{group}' ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", f"Time series graph could not be generated for group '{group}' due to insufficient data."))
                                    else:
                                        st.warning(get_localized_text(f"ã‚°ãƒ«ãƒ¼ãƒ— '{group}' ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", f"No data for group '{group}'."))
                            
                            if not has_data_to_plot:
                                st.warning(get_localized_text("é¸æŠã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—ã®ã„ãšã‚Œã‚‚æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒƒãƒˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚", "No time series data could be plotted for any of the selected groups."))
                        else:
                            st.warning(get_localized_text(f"é¸æŠã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—åˆ— '{trend_group}' ã«æœ‰åŠ¹ãªå€¤ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", f"No valid values in selected group column '{trend_group}'."))
                    else:
                        st.error(get_localized_text(f"é¸æŠã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—åˆ— '{trend_group}' ãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚", f"Selected group column '{trend_group}' does not exist in the dataframe."))

                if has_data_to_plot:
                    ax.set_title(get_graph_text(
                        f"{trend_metric}ã®æ™‚ç³»åˆ— ({'å…¨ä½“' if trend_group == 'ãªã—' else trend_group}åˆ¥, fontproperties=font_prop)",
                        f"Time Series of {trend_metric} (by {'Overall' if trend_group == 'ãªã—' else trend_group})"
                    ), fontfamily=plt.rcParams['font.family'], fontsize=16)
                    ax.set_xlabel(get_graph_text("å®Ÿæ–½æ—¥", "Date", fontproperties=font_prop), fontfamily=plt.rcParams['font.family'], fontsize=12)
                    ax.set_ylabel(get_graph_text(f"{trend_metric} ({agg_period_display}å¹³å‡, fontproperties=font_prop)", f"{trend_metric} ({agg_period_display} Average)"), fontfamily=plt.rcParams['font.family'], fontsize=12)
                    ax.legend(prop={'family':plt.rcParams['font.family']}, fontproperties=font_prop)
                    plt.tight_layout()
                    st.pyplot(fig)
                else:
                    plt.close(fig) 


            except Exception as e:
                st.error(get_localized_text(f"æ™‚ç³»åˆ—åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", f"An error occurred during time series analysis: {e}"))
        else:
            st.warning(get_localized_text("ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¿ãƒ–ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "No data uploaded or no data after filtering. Please upload files in the Data Management tab."))


    with tabs[5]:
        st.header(get_localized_text("ğŸ† ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æ", "ğŸ† Ranking Analysis"))

        if 'current_data' in st.session_state and st.session_state.current_data is not None and not st.session_state.current_data.empty:
            df = st.session_state.current_data.copy()

            numeric_cols = df.select_dtypes(include='number').columns.tolist()
            cat_cols = df.select_dtypes(include='object').columns.tolist()

            if not numeric_cols:
                st.warning(get_localized_text("æ•°å€¤åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "No numeric columns found."))
                st.stop()
            if not cat_cols:
                st.warning(get_localized_text("ã‚«ãƒ†ã‚´ãƒªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "No categorical columns found."))
                st.stop()

            col1, col2 = st.columns(2)
            with col1:
                rank_metric = st.selectbox(
                    get_localized_text("ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯¾è±¡ã®æŒ‡æ¨™", "Metric for Ranking"), 
                    numeric_cols,
                    key="rank_metric"
                )
                rank_group = st.selectbox(
                    get_localized_text("ã‚°ãƒ«ãƒ¼ãƒ—åŒ–", "Group By"),
                    cat_cols,
                    key="rank_group"
                )

            with col2:
                top_n = st.number_input(
                    get_localized_text("è¡¨ç¤ºä»¶æ•°", "Number of Items to Display"),
                    min_value=1,
                    max_value=50,
                    value=10,
                    key="rank_topn"
                )
                ascending_option = st.radio(
                    get_localized_text("ä¸¦ã³é †", "Sort Order"),
                    [get_localized_text("é™é †ï¼ˆå¤§ãã„é †ï¼‰", "Descending (Largest First)"), get_localized_text("æ˜‡é †ï¼ˆå°ã•ã„é †ï¼‰", "Ascending (Smallest First)")],
                    index=0,
                    key="rank_order"
                )

            if st.button(get_localized_text("ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¡¨ç¤º", "Show Ranking"), key="rank_execute"):
                try:
                    if rank_group not in df.columns or rank_metric not in df.columns:
                        st.error(get_localized_text("é¸æŠã•ã‚ŒãŸåˆ—ãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚", "Selected columns do not exist in the dataframe."))
                        st.stop()

                    rank_df = df.groupby(rank_group)[rank_metric].agg(['mean', 'count']).round(2)
                    rank_df.columns = [get_localized_text('å¹³å‡å€¤', 'Mean Value'), get_localized_text('ãƒ‡ãƒ¼ã‚¿æ•°', 'Data Count')]
                    rank_df = rank_df.sort_values(
                        get_localized_text('å¹³å‡å€¤', 'Mean Value'),
                        ascending=(ascending_option == get_localized_text("æ˜‡é †ï¼ˆå°ã•ã„é †ï¼‰", "Ascending (Smallest First)"))
                    )

                    st.subheader(get_localized_text("ğŸ“Š ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœ", "ğŸ“Š Ranking Results"))
                    rank_display = rank_df.head(top_n).copy()
                    rank_display.index.name = rank_group
                    st.dataframe(rank_display)

                    fig, ax = plt.subplots(figsize=(10, max(5, top_n * 0.3)))
                    rank_display[get_localized_text('å¹³å‡å€¤', 'Mean Value')].plot(kind='barh', ax=ax)
                    ax.set_title(get_graph_text(f"{rank_group}åˆ¥ {rank_metric}ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°", f"Ranking of {rank_metric} by {rank_group}", fontproperties=font_prop), fontfamily=plt.rcParams['font.family'], fontsize=16)
                    ax.set_xlabel(get_graph_text(f"{rank_metric} å¹³å‡å€¤", f"{rank_metric} Mean Value", fontproperties=font_prop), fontfamily=plt.rcParams['font.family'], fontsize=12)
                    ax.set_ylabel(get_graph_text(rank_group, rank_group, fontproperties=font_prop), fontfamily=plt.rcParams['font.family'], fontsize=12) # Group name is data, keep as is
                    
                    for label in ax.get_xticklabels():
                        label.set_fontfamily(plt.rcParams['font.family'])
                    for label in ax.get_yticklabels():
                        label.set_fontfamily(plt.rcParams['font.family'])

                    plt.tight_layout()
                    st.pyplot(fig)

                    csv = rank_display.to_csv(encoding='utf-8-sig').encode('utf-8-sig')
                    st.download_button(
                        get_localized_text("ğŸ“¥ ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ä¿å­˜", "ğŸ“¥ Save Ranking Data as CSV"),
                        csv,
                        file_name="ranking_data.csv",
                        mime='text/csv',
                        key="rank_download"
                    )

                    st.subheader(get_localized_text("ğŸ“ˆ ç‰¹å¾´çš„ãªãƒ‡ãƒ¼ã‚¿", "ğŸ“ˆ Characteristic Data"))

                    if not rank_display.empty:
                        top_item = rank_display.index[0]
                        top_val = rank_display.loc[top_item, get_localized_text('å¹³å‡å€¤', 'Mean Value')]
                        st.info(get_localized_text(
                            f"ğŸ† ãƒˆãƒƒãƒ—ã®{rank_group}: {top_item} ({top_val:.2f})",
                            f"ğŸ† Top {rank_group}: {top_item} ({top_val:.2f})"
                        ))

                        if len(rank_display) > 1:
                            second_item = rank_display.index[1]
                            second_val = rank_display.loc[second_item, get_localized_text('å¹³å‡å€¤', 'Mean Value')]
                            if pd.notna(top_val) and pd.notna(second_val):
                                diff = top_val - second_val
                                if second_val != 0:
                                    st.info(get_localized_text(
                                        f"2ä½ã¨ã®å·®: {diff:.2f} ({(diff/second_val*100):.1f}%)",
                                        f"Difference from 2nd place: {diff:.2f} ({(diff/second_val*100):.1f}%)"
                                    ))
                                else:
                                    st.info(get_localized_text(
                                        f"2ä½ã¨ã®å·®: {diff:.2f} (2ä½ã®å€¤ãŒ0ã®ãŸã‚å¤‰åŒ–ç‡ã‚’è¨ˆç®—ã§ãã¾ã›ã‚“)",
                                        f"Difference from 2nd place: {diff:.2f} (Cannot calculate percentage change as 2nd place value is 0)"
                                    ))
                            else:
                                st.info(get_localized_text("ãƒˆãƒƒãƒ—ã¾ãŸã¯2ä½ã®å€¤ãŒæ¬ æã—ã¦ã„ã‚‹ãŸã‚ã€å·®ã‚’è¨ˆç®—ã§ãã¾ã›ã‚“ã€‚", "Cannot calculate difference as top or second place value is missing."))
                    else:
                        st.info(get_localized_text("ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ç‰¹å¾´çš„ãªãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã€‚", "Ranking data is empty, cannot identify characteristic data."))

                except Exception as e:
                    st.error(get_localized_text(f"ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", f"An error occurred during ranking analysis: {e}"))
        else:
            st.warning(get_localized_text("ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¿ãƒ–ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "No data uploaded or no data after filtering. Please upload files in the Data Management tab."))

    with tabs[6]:
        st.header(get_localized_text("ğŸ“‹ è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆ", "ğŸ“‹ Automatic Report"))

        if 'current_data' in st.session_state and st.session_state.current_data is not None and not st.session_state.current_data.empty:
            df = st.session_state.current_data.copy()

            if 'å‚åŠ ç‡(%)' not in df.columns and 'ç”³è¾¼æ•°' in df.columns and 'å‚åŠ è€…æ•°' in df.columns:
                df['å‚åŠ ç‡(%)'] = (df['å‚åŠ è€…æ•°'] / df['ç”³è¾¼æ•°']) * 100
            if 'æº€è¶³ç‡(%)' not in df.columns and 'æº€è¶³å›ç­”' in df.columns and 'å‚åŠ è€…æ•°' in df.columns:
                df['æº€è¶³ç‡(%)'] = (df['æº€è¶³å›ç­”'] / df['å‚åŠ è€…æ•°']) * 100
            # ä¿®æ­£: 'not in in' ã‚’ 'not in' ã«å¤‰æ›´
            if 'ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‡' not in df.columns and 'å‚åŠ è€…æ•°' in df.columns:
                df['ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‡'] = df['ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°'] / df['å‚åŠ è€…æ•°']

            df = DataProcessor.expand_time_slots(df)

            st.subheader(get_localized_text("ğŸ“£ å‚åŠ è€…æ•°ã‚’å¢—ã‚„ã™ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿åˆ†æ", "ğŸ“£ Data Analysis for Increasing Participants"))

            def append_section_to_report(title_jp, df_to_use):
                title_en = ""
                if title_jp == "å‚åŠ è€…æ•°ãŒå¤šã„ãƒãƒ¼ãƒ ":
                    title_en = "Teams with High Participants"
                elif title_jp == "æ›œæ—¥åˆ¥ã®å‚åŠ è€…æ•°":
                    title_en = "Participants by Day of Week"
                elif title_jp == "æ™‚é–“å¸¯åˆ¥ã®å‚åŠ è€…æ•°":
                    title_en = "Participants by Time Slot"
                elif title_jp == "å‚åŠ ç‡ãŒé«˜ã„ã‚¤ãƒ™ãƒ³ãƒˆ":
                    title_en = "Events with High Participation Rate"
                elif title_jp == "æº€è¶³åº¦ãŒé«˜ã„ã‚¤ãƒ™ãƒ³ãƒˆ":
                    title_en = "Events with High Satisfaction Rate"
                elif title_jp == "å‚åŠ è€…æ•°ãŒå°‘ãªã„æ›œæ—¥":
                    title_en = "Weekdays with Low Participants"
                
                st.markdown(get_localized_text(f"#### {title_jp}", f"#### {title_en}"))
                st.dataframe(df_to_use, use_container_width=True) # use_container_width=True ã‚’è¿½åŠ 


            st.markdown(get_localized_text("### ğŸ† ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¾ã¨ã‚", "### ğŸ† Ranking Summary"))
            if 'æ‹…å½“ãƒãƒ¼ãƒ ' in df.columns and 'å‚åŠ è€…æ•°' in df.columns:
                team_avg = df.groupby("æ‹…å½“ãƒãƒ¼ãƒ ")["å‚åŠ è€…æ•°"].mean().sort_values(ascending=False).reset_index()
                append_section_to_report("å‚åŠ è€…æ•°ãŒå¤šã„ãƒãƒ¼ãƒ ", team_avg)
            else:
                st.info(get_localized_text("ã€Œæ‹…å½“ãƒãƒ¼ãƒ ã€ã¾ãŸã¯ã€Œå‚åŠ è€…æ•°ã€ã®åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "The 'æ‹…å½“ãƒãƒ¼ãƒ ' (Responsible Team) or 'å‚åŠ è€…æ•°' (Participants) column is missing."))

            if 'æ›œæ—¥' in df.columns and 'å‚åŠ è€…æ•°' in df.columns:
                weekday_avg = df.groupby("æ›œæ—¥")["å‚åŠ è€…æ•°"].mean().sort_values(ascending=False).reindex(index=['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']).dropna().reset_index()
                append_section_to_report("æ›œæ—¥åˆ¥ã®å‚åŠ è€…æ•°", weekday_avg)
            else:
                st.info(get_localized_text("ã€Œæ›œæ—¥ã€ã¾ãŸã¯ã€Œå‚åŠ è€…æ•°ã€ã®åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "The 'æ›œæ—¥' (Weekday) or 'å‚åŠ è€…æ•°' (Participants) column is missing."))

            if 'æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ' in df.columns and 'å‚åŠ è€…æ•°' in df.columns:
                time_avg = df.groupby("æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ")["å‚åŠ è€…æ•°"].mean().sort_values(ascending=False).reset_index()
                append_section_to_report("æ™‚é–“å¸¯åˆ¥ã®å‚åŠ è€…æ•°", time_avg)
            else:
                st.info(get_localized_text("ã€Œæ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆã€ã¾ãŸã¯ã€Œå‚åŠ è€…æ•°ã€ã®åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "The 'æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ' (Time Slot) or 'å‚åŠ è€…æ•°' (Participants) column is missing."))


            if 'å‚åŠ ç‡(%)' in df.columns:
                cols_for_top_rate = [col for col in ["ã‚¤ãƒ™ãƒ³ãƒˆå", "æ›œæ—¥", "æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ", "å‚åŠ ç‡(%)"] if col in df.columns]
                top_rate = df.sort_values("å‚åŠ ç‡(%)", ascending=False).head(5)[cols_for_top_rate].reset_index(drop=True)
                top_rate.index += 1
                append_section_to_report("å‚åŠ ç‡ãŒé«˜ã„ã‚¤ãƒ™ãƒ³ãƒˆ", top_rate)
            else:
                st.info(get_localized_text("ã€Œå‚åŠ ç‡(%)ã€ã®åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "The 'å‚åŠ ç‡(%)' (Participation Rate (%)) column is missing."))

            if 'æº€è¶³ç‡(%)' in df.columns:
                cols_for_top_satisfaction = [col for col in ["ã‚¤ãƒ™ãƒ³ãƒˆå", "æ›œæ—¥", "æ™‚é–“å¸¯ã‚¹ãƒ­ãƒƒãƒˆ", "æº€è¶³ç‡(%)"] if col in df.columns]
                top_satisfaction = df.sort_values("æº€è¶³ç‡(%)", ascending=False).head(5)[cols_for_top_satisfaction].reset_index(drop=True)
                top_satisfaction.index += 1
                append_section_to_report("æº€è¶³åº¦ãŒé«˜ã„ã‚¤ãƒ™ãƒ³ãƒˆ", top_satisfaction)
            else:
                st.info(get_localized_text("ã€Œæº€è¶³ç‡(%)ã€ã®åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "The 'æº€è¶³ç‡(%)' (Satisfaction Rate (%)) column is missing."))

            if 'æ›œæ—¥' in df.columns and 'å‚åŠ è€…æ•°' in df.columns:
                low_participation_raw = df.groupby("æ›œæ—¥")["å‚åŠ è€…æ•°"].mean().sort_values().reset_index()
                low_participation = low_participation_raw.head(3).copy()
                low_participation.index += 1
                append_section_to_report("å‚åŠ è€…æ•°ãŒå°‘ãªã„æ›œæ—¥", low_participation)
            else:
                st.info(get_localized_text("ã€Œæ›œæ—¥ã€ã¾ãŸã¯ã€Œå‚åŠ è€…æ•°ã€ã®åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", "The 'æ›œæ—¥' (Weekday) or 'å‚åŠ è€…æ•°' (Participants) column is missing."))

            st.markdown(get_localized_text("### ğŸ’¡ å®£ä¼ãƒ»ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨å‚åŠ è€…æ•°ã®é–¢ä¿‚", "### ğŸ’¡ Relationship between Promotion/Reactions and Participants"))
            
            corr_summary_text = []
            if 'å®£ä¼å›æ•°' in df.columns and 'å‚åŠ è€…æ•°' in df.columns:
                corr1 = df['å‚åŠ è€…æ•°'].corr(df['å®£ä¼å›æ•°'])
                if pd.notna(corr1):
                    corr_summary_text.append(get_localized_text(f"ã€Œå®£ä¼å›æ•°ã€ã¨ã€Œå‚åŠ è€…æ•°ã€ã®ç›¸é–¢: {corr1:.2f}", f"Correlation between 'å®£ä¼å›æ•°' (Promotion Count) and 'å‚åŠ è€…æ•°' (Participants): {corr1:.2f}"))
                else:
                    corr_summary_text.append(get_localized_text("ã€Œå®£ä¼å›æ•°ã€ã¨ã€Œå‚åŠ è€…æ•°ã€ã®ç›¸é–¢ã¯è¨ˆç®—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¾ãŸã¯å®šæ•°ï¼‰ã€‚", "Correlation between 'å®£ä¼å›æ•°' (Promotion Count) and 'å‚åŠ è€…æ•°' (Participants) could not be calculated (insufficient data or constant)."))
            else:
                corr_summary_text.append(get_localized_text("ã€Œå®£ä¼å›æ•°ã€ã®åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", "The 'å®£ä¼å›æ•°' (Promotion Count) column was not found."))

            if 'ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‡' in df.columns and 'å‚åŠ è€…æ•°' in df.columns:
                corr2 = df['å‚åŠ è€…æ•°'].corr(df['ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‡'])
                if pd.notna(corr2):
                    corr_summary_text.append(get_localized_text(f"ã€Œãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‡ã€ã¨ã€Œå‚åŠ è€…æ•°ã€ã®ç›¸é–¢: {corr2:.2f}", f"Correlation between 'ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‡' (Reaction Rate) and 'å‚åŠ è€…æ•°' (Participants): {corr2:.2f}"))
                else:
                    corr_summary_text.append(get_localized_text("ã€Œãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‡ã€ã¨ã€Œå‚åŠ è€…æ•°ã€ã®ç›¸é–¢ã¯è¨ˆç®—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¾ãŸã¯å®šæ•°ï¼‰ã€‚", "Correlation between 'ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‡' (Reaction Rate) and 'å‚åŠ è€…æ•°' (Participants) could not be calculated (insufficient data or constant)."))
            else:
                st.info(get_localized_text("ã€Œãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‡ã€ã®åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", "The 'ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‡' (Reaction Rate) column was not found."))
            
            for line in corr_summary_text:
                st.info(line)


        else:
            st.warning(get_localized_text("ãƒ‡ãƒ¼ã‚¿ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¿ãƒ–ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "No data uploaded or no data after filtering. Please upload files in the Data Management tab."))


# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ ---
def main():
    # Streamlitã®ãƒšãƒ¼ã‚¸è¨­å®šã¯ä¸€åº¦ã ã‘è¡Œã†
    st.set_page_config(page_title=get_localized_text("VRã‚¤ãƒ™ãƒ³ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«", "VR Event Analysis Tool"), layout="wide")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«logged_inãŒãªã‘ã‚Œã°åˆæœŸåŒ–ï¼ˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ã®ã¿Falseã«è¨­å®šï¼‰
    if "logged_in" not in st.session_state:
        st.session_state["logged_in"] = False

    if st.session_state.get("logged_in"):
        # ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã®å ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’è¡¨ç¤ºã—ã€ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³ã¨ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚’è¡¨ç¤º
        st.sidebar.markdown(get_localized_text(f"**ã‚ˆã†ã“ãã€{st.session_state.get('username')} ã•ã‚“ï¼**", f"**Welcome, {st.session_state.get('username')}!**"))
        if st.sidebar.button(get_localized_text("ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ", "Logout")):
            # ãƒ­ã‚°ã‚¢ã‚¦ãƒˆå‡¦ç†
            for key in ["logged_in", "username", "num_uploaders"]: # num_uploadersã‚‚ã‚¯ãƒªã‚¢
                st.session_state.pop(key, None)
            st.rerun()
        show_main_app()
    else:
        # æœªãƒ­ã‚°ã‚¤ãƒ³ã®å ´åˆã€ãƒ­ã‚°ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ ã‚’è¡¨ç¤º
        login_form()

if __name__ == "__main__":
    main()
